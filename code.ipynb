{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\"\"\"\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='', train=True, transform=trans, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "a=iter(train_loader)\n",
    "print(a.next()[0].shape)\n",
    "exit(0)\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class CaptionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, data_name,data_type='None', transform=None):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where data files are stored\n",
    "        :param data_name: base name of processed datasets\n",
    "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
    "        :param transform: image transform pipeline\n",
    "        \"\"\"\n",
    "\n",
    "        # Open hdf5 file where images are stored\n",
    "        add=''\n",
    "        if data_type=='VAL':\n",
    "            add='_VAL' \n",
    "        self.h = h5py.File(os.path.join(data_folder, data_name + add + '.h5'), 'r')\n",
    "        self.imgs = self.h['X_train']\n",
    "        #print(self.imgs[4232])\n",
    "        #self.imgs=torch.Tensor(self.imgs)\n",
    "        #self.imgs=self.imgs.resize((21223,1,256,256))\n",
    "        self.split = data_type\n",
    "        # Captions per image\n",
    "        self.value = self.h['Y_train']\n",
    "        self.value=np.array(self.value)\n",
    "        self.value=np.array([np.where(r==1)[0][0] for r in self.value])\n",
    "        # PyTorch transformation pipeline for the image (normalizing, etc.)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.value)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
    "        img = torch.FloatTensor(self.imgs[i] / 255.)\n",
    "        value=torch.LongTensor([self.value[i]])\n",
    "        img=img.unsqueeze(0)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        #print(img.shape)\n",
    "        return img,value.squeeze(0)\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "\n",
    "data_folder = 'D:\\REMBRANDT'\n",
    "data_name='data'\n",
    "batch_size=32\n",
    "workers=0\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "normalize=transforms.Normalize((0.1307,), (0.3081,))\n",
    "\n",
    "dataset=CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize]))\n",
    "train, val = torch.utils.data.random_split(dataset, [20000,1223])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train,\n",
    "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "val_loader =  torch.utils.data.DataLoader(\n",
    "        val,\n",
    "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "#print(iter(val_loader).next()[0].shape)\n",
    "#exit(0)\n",
    "#print(iter(train_loader).next())\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(65536, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 3)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        #print(\"out\",out.shape)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/625], Loss: 0.7171, Accuracy: 68.75%\n",
      "Epoch [1/10], Step [200/625], Loss: 0.7272, Accuracy: 71.88%\n",
      "Epoch [1/10], Step [300/625], Loss: 0.9253, Accuracy: 50.00%\n",
      "Epoch [1/10], Step [400/625], Loss: 0.5597, Accuracy: 78.12%\n",
      "Epoch [1/10], Step [500/625], Loss: 0.7054, Accuracy: 65.62%\n",
      "Epoch [1/10], Step [600/625], Loss: 0.5425, Accuracy: 78.12%\n",
      "Epoch [2/10], Step [100/625], Loss: 0.6840, Accuracy: 65.62%\n",
      "Epoch [2/10], Step [200/625], Loss: 0.8232, Accuracy: 65.62%\n",
      "Epoch [2/10], Step [300/625], Loss: 0.7665, Accuracy: 65.62%\n",
      "Epoch [2/10], Step [400/625], Loss: 0.6488, Accuracy: 71.88%\n",
      "Epoch [2/10], Step [500/625], Loss: 0.4778, Accuracy: 84.38%\n",
      "Epoch [2/10], Step [600/625], Loss: 0.6679, Accuracy: 81.25%\n",
      "Epoch [3/10], Step [100/625], Loss: 0.5742, Accuracy: 71.88%\n",
      "Epoch [3/10], Step [200/625], Loss: 0.4404, Accuracy: 81.25%\n",
      "Epoch [3/10], Step [300/625], Loss: 0.7090, Accuracy: 71.88%\n",
      "Epoch [3/10], Step [400/625], Loss: 0.4549, Accuracy: 81.25%\n",
      "Epoch [3/10], Step [500/625], Loss: 0.5172, Accuracy: 84.38%\n",
      "Epoch [3/10], Step [600/625], Loss: 0.5667, Accuracy: 78.12%\n",
      "Epoch [4/10], Step [100/625], Loss: 0.5287, Accuracy: 81.25%\n",
      "Epoch [4/10], Step [200/625], Loss: 0.5253, Accuracy: 75.00%\n",
      "Epoch [4/10], Step [300/625], Loss: 0.4827, Accuracy: 84.38%\n",
      "Epoch [4/10], Step [400/625], Loss: 0.4280, Accuracy: 84.38%\n",
      "Epoch [4/10], Step [500/625], Loss: 0.3942, Accuracy: 87.50%\n",
      "Epoch [4/10], Step [600/625], Loss: 0.5231, Accuracy: 75.00%\n",
      "Epoch [5/10], Step [100/625], Loss: 0.4461, Accuracy: 87.50%\n",
      "Epoch [5/10], Step [200/625], Loss: 0.5658, Accuracy: 68.75%\n",
      "Epoch [5/10], Step [300/625], Loss: 1.3135, Accuracy: 65.62%\n",
      "Epoch [5/10], Step [400/625], Loss: 0.5057, Accuracy: 75.00%\n",
      "Epoch [5/10], Step [500/625], Loss: 0.4115, Accuracy: 81.25%\n",
      "Epoch [5/10], Step [600/625], Loss: 0.6811, Accuracy: 65.62%\n",
      "Epoch [6/10], Step [100/625], Loss: 0.3815, Accuracy: 81.25%\n",
      "Epoch [6/10], Step [200/625], Loss: 0.2249, Accuracy: 87.50%\n",
      "Epoch [6/10], Step [300/625], Loss: 0.3903, Accuracy: 84.38%\n",
      "Epoch [6/10], Step [400/625], Loss: 0.5039, Accuracy: 78.12%\n",
      "Epoch [6/10], Step [500/625], Loss: 0.3237, Accuracy: 90.62%\n",
      "Epoch [6/10], Step [600/625], Loss: 0.3211, Accuracy: 90.62%\n",
      "Epoch [7/10], Step [100/625], Loss: 0.2891, Accuracy: 90.62%\n",
      "Epoch [7/10], Step [200/625], Loss: 0.4255, Accuracy: 84.38%\n",
      "Epoch [7/10], Step [300/625], Loss: 0.2946, Accuracy: 81.25%\n",
      "Epoch [7/10], Step [400/625], Loss: 0.4102, Accuracy: 84.38%\n",
      "Epoch [7/10], Step [500/625], Loss: 0.2768, Accuracy: 87.50%\n",
      "Epoch [7/10], Step [600/625], Loss: 0.3350, Accuracy: 87.50%\n",
      "Epoch [8/10], Step [100/625], Loss: 0.3405, Accuracy: 93.75%\n",
      "Epoch [8/10], Step [200/625], Loss: 0.3074, Accuracy: 90.62%\n",
      "Epoch [8/10], Step [300/625], Loss: 0.2390, Accuracy: 90.62%\n",
      "Epoch [8/10], Step [400/625], Loss: 0.4303, Accuracy: 84.38%\n",
      "Epoch [8/10], Step [500/625], Loss: 0.2577, Accuracy: 84.38%\n",
      "Epoch [8/10], Step [600/625], Loss: 0.4384, Accuracy: 78.12%\n",
      "Epoch [9/10], Step [100/625], Loss: 0.3838, Accuracy: 87.50%\n",
      "Epoch [9/10], Step [200/625], Loss: 0.1576, Accuracy: 96.88%\n",
      "Epoch [9/10], Step [300/625], Loss: 0.3800, Accuracy: 81.25%\n",
      "Epoch [9/10], Step [400/625], Loss: 0.3314, Accuracy: 87.50%\n",
      "Epoch [9/10], Step [500/625], Loss: 0.5893, Accuracy: 78.12%\n",
      "Epoch [9/10], Step [600/625], Loss: 0.2432, Accuracy: 87.50%\n",
      "Epoch [10/10], Step [100/625], Loss: 0.1674, Accuracy: 93.75%\n",
      "Epoch [10/10], Step [200/625], Loss: 0.3734, Accuracy: 84.38%\n",
      "Epoch [10/10], Step [300/625], Loss: 0.3527, Accuracy: 84.38%\n",
      "Epoch [10/10], Step [400/625], Loss: 0.4171, Accuracy: 84.38%\n",
      "Epoch [10/10], Step [500/625], Loss: 0.3395, Accuracy: 90.62%\n",
      "Epoch [10/10], Step [600/625], Loss: 0.2916, Accuracy: 87.50%\n",
      "Training Done...\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.001\n",
    "num_epochs=10\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "model=model.to(device)\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        outputs = model(images.to(device))\n",
    "        #print(outputs[0])\n",
    "        #print(labels[0])\n",
    "        loss = criterion(outputs.to(device), labels.to(device))\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels.to(device)).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "\n",
    "print(\"Training Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: \n",
      " Loss: 0.3626, Accuracy: 84.38%\n",
      " Loss: 0.5269, Accuracy: 84.38%\n",
      " Loss: 0.3915, Accuracy: 87.50%\n",
      "torch.Size([1223])\n",
      "tensor([[162,  48,  14],\n",
      "        [ 61, 720,  39],\n",
      "        [ 18,  41, 120]])\n",
      "Confusion matrix, without normalization\n",
      "tensor([[162,  48,  14],\n",
      "        [ 61, 720,  39],\n",
      "        [ 18,  41, 120]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[0.7255, 0.1616, 0.1128]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy: \")\n",
    "# test=iter(val_loader).next()[0][0].unsqueeze(0)\n",
    "# print(test.shape)\n",
    "# output=model(test.to(device))\n",
    "# print(torch.softmax(output,dim=1).argmax(dim=1))\n",
    "# exit(0)\n",
    "total_pred=torch.LongTensor([])\n",
    "total_label=torch.LongTensor([])\n",
    "for i, (images,labels) in enumerate(val_loader):\n",
    "    outputs=model(images.to(device))\n",
    "    #print(torch.softmax(outputs,dim=1).argmax(dim=1))\n",
    "    #print(labels)\n",
    "    loss=criterion(outputs.to(device),labels.to(device))\n",
    "    outputs=torch.softmax(outputs,dim=1).argmax(dim=1)\n",
    "    total = labels.size(0)\n",
    "    total_pred=torch.cat((total_pred.to(device),outputs),0)\n",
    "    total_label=torch.cat((total_label.to(device),labels.to(device)),0)\n",
    "    #_, predicted = torch.max(outputs.data, 1)\n",
    "    predicted=outputs\n",
    "    correct = (predicted == labels.to(device)).sum().item()\n",
    "    acc_list.append(correct / total)  \n",
    "    if (i+1)%10 == 0: \n",
    "        print(' Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                    .format(loss.item(),\n",
    "                            (correct / total) * 100))\n",
    "print(total_pred.shape)\n",
    "stacked = torch.stack(\n",
    "    (\n",
    "        total_label\n",
    "        ,total_pred\n",
    "    )\n",
    "    ,dim=1\n",
    ")\n",
    "cmt = torch.zeros(3,3, dtype=torch.int64)\n",
    "for p in stacked:\n",
    "    tl, pl = p.tolist()\n",
    "    cmt[tl, pl] = cmt[tl, pl] + 1\n",
    "print(cmt)\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "plt.figure(figsize=(3,3))\n",
    "names=('class1','class2','class3')\n",
    "plot_confusion_matrix(cmt, names)\n",
    "plt.show()\n",
    "\n",
    "##### Testing using individual images #######\n",
    "##### If you are reading an image externally ###\n",
    "##### make sure you convert it to tensor of dimension: ###\n",
    "#####  [1,1,256,256]. ###\n",
    "\n",
    "test=iter(val_loader).next()[0][0].unsqueeze(0)\n",
    "print(test.shape)\n",
    "output=model(test.to(device))\n",
    "print(torch.softmax(output,dim=1))\n",
    "print(torch.softmax(output,dim=1).argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
